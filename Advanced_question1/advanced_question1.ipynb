{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNs2nbFPEUVc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()#weights didnt update\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" #dimensionality check\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        Q = self.W_q(x)#what to look for or query\n",
        "        K = self.W_k(x)#the keys or important parts\n",
        "        V = self.W_v(x)#values for the keys\n",
        "\n",
        "        Q = Q.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)#formula = (Q.K)/(d^0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)#softmaxxing\n",
        "\n",
        "        out = attn @ V#final output\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
        "\n",
        "        return self.W_o(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#causal mask used by gpt-2\n",
        "def causal_mask(seq_len):\n",
        "    \"\"\"\n",
        "    Returns shape: (1, 1, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    return torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)# to let it access only prev vals\n"
      ],
      "metadata": {
        "id": "FE6cTGrqEZYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)#position matrix\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)#to get positions\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)#for even terms\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)#for odd terms\n",
        "\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "lOcK3Vj9EbBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#random input i got from chatgpt to test the code\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Parameters\n",
        "    batch_size = 2\n",
        "    seq_len = 5\n",
        "    d_model = 8\n",
        "    num_heads = 2\n",
        "\n",
        "    # Dummy input (like embeddings)\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    print(\"Input shape:\", x.shape)\n",
        "\n",
        "    # Positional Encoding\n",
        "    pos_enc = PositionalEncoding(d_model)\n",
        "    x_pos = pos_enc(x)\n",
        "\n",
        "    print(\"After Positional Encoding:\", x_pos.shape)\n",
        "\n",
        "    # Multi-Head Attention\n",
        "    mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    mask = causal_mask(seq_len)  # try with and without mask\n",
        "    out = mha(x_pos, mask)\n",
        "\n",
        "    print(\"After Multi-Head Attention:\", out.shape)\n",
        "\n",
        "    print(\"\\nSample output tensor:\\n\", out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4MZaZCrGJmH",
        "outputId": "5a24cf99-2fc5-4966-d660-11a6bc370992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 5, 8])\n",
            "After Positional Encoding: torch.Size([2, 5, 8])\n",
            "After Multi-Head Attention: torch.Size([2, 5, 8])\n",
            "\n",
            "Sample output tensor:\n",
            " tensor([[[-0.0132, -0.9266,  0.0755, -0.2064, -0.4950,  0.3838, -0.4032,\n",
            "           0.0495],\n",
            "         [-0.3515, -0.5994,  0.3107, -0.1495, -0.2342, -0.0237, -0.0849,\n",
            "          -0.2068],\n",
            "         [-0.6619, -0.5558,  0.5254, -0.3041,  0.1050, -0.3706,  0.0107,\n",
            "          -0.4898],\n",
            "         [-0.8187, -0.4154,  0.5876, -0.4875,  0.2922, -0.5425, -0.0012,\n",
            "          -0.5605],\n",
            "         [-0.5957, -0.2417,  0.3288, -0.3884,  0.1129, -0.3773, -0.0256,\n",
            "          -0.3239]],\n",
            "\n",
            "        [[-0.6042, -0.1195,  0.3992, -0.4868,  0.0033, -0.5287,  0.2096,\n",
            "          -0.3515],\n",
            "         [-0.7286, -0.1981,  0.5124, -0.6597,  0.1962, -0.5426,  0.0217,\n",
            "          -0.4896],\n",
            "         [-0.6133, -0.3381,  0.4341, -0.6199,  0.0887, -0.4186, -0.0337,\n",
            "          -0.3343],\n",
            "         [-0.7838, -0.3363,  0.5110, -0.7009,  0.1155, -0.6016,  0.0721,\n",
            "          -0.4981],\n",
            "         [-0.9728,  0.0287,  0.4274, -0.7321,  0.3184, -0.9205,  0.2333,\n",
            "          -0.6042]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}